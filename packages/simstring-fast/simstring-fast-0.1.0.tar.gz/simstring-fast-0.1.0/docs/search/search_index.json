{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Simstring Getting started Install with pip install simstring-fast from simstring.feature_extractor import CharacterNgramFeatureExtractor from simstring.measure import CosineMeasure from simstring.database import DictDatabase from simstring.searcher import Searcher db = DictDatabase(CharacterNgramFeatureExtractor(2)) db.add('foo') db.add('bar') db.add('fooo') searcher = Searcher(db, CosineMeasure()) results = searcher.search('foo', 0.8) print(results) Simstring The original method is described in this paper . There is an even faster C++ implimentation by the original authors available here This module is a fork of this repo which is no longer actively maintained. This module adds documentation, speedups and more measures and features such saving compiled databases. Banchmarks Without compilation the code takes 14 seconds to run through this particular banchmark, which is only on the data retrieval. With compiltion this time is dropped to below 5 seconds.","title":"Home"},{"location":"#simstring","text":"","title":"Simstring"},{"location":"#getting-started","text":"Install with pip install simstring-fast from simstring.feature_extractor import CharacterNgramFeatureExtractor from simstring.measure import CosineMeasure from simstring.database import DictDatabase from simstring.searcher import Searcher db = DictDatabase(CharacterNgramFeatureExtractor(2)) db.add('foo') db.add('bar') db.add('fooo') searcher = Searcher(db, CosineMeasure()) results = searcher.search('foo', 0.8) print(results)","title":"Getting started"},{"location":"#simstring_1","text":"The original method is described in this paper . There is an even faster C++ implimentation by the original authors available here This module is a fork of this repo which is no longer actively maintained. This module adds documentation, speedups and more measures and features such saving compiled databases.","title":"Simstring"},{"location":"#banchmarks","text":"Without compilation the code takes 14 seconds to run through this particular banchmark, which is only on the data retrieval. With compiltion this time is dropped to below 5 seconds.","title":"Banchmarks"},{"location":"database/","text":"Database Dict based database Bases: BaseDatabase Source code in simstring\\database\\dict.py class DictDatabase(BaseDatabase): def __init__(self, feature_extractor): self.feature_extractor = feature_extractor self.strings: List[str] = [] self.feature_set_size_to_string_map: Dict[int, Set[str]] = defaultdict(set) # 3.10 and up only self.feature_set_size_and_feature_to_string_map: dict = defaultdict(defaultdict_set) def add(self, string: str): features = self.feature_extractor.features(string) size = len(features) self.strings.append(string) self.feature_set_size_to_string_map[size].add(string) for feature in features: self.feature_set_size_and_feature_to_string_map[size][feature].add(string) def all(self) -> List[str]: return self.strings def lookup_strings_by_feature_set_size_and_feature(self, size: int, feature: str) -> Set[str]: return self.feature_set_size_and_feature_to_string_map[size][feature] def min_feature_size(self) -> int: return min(self.feature_set_size_to_string_map.keys()) def max_feature_size(self) -> int: return max(self.feature_set_size_to_string_map.keys()) # def __getstate__(self): # \"\"\"To pickle the object\"\"\" # return self.__dict__ # def __setstate__(self, d): # \"\"\"To unpickle the object\"\"\" # self.__dict__ = d def save(self, filename:str): \"\"\"Save the database to a file as defined by filename. Args: filename: Filename to save the db at. Should include file extention. Returns: None \"\"\" with open(filename, \"wb\") as f: pickle.dump(self, f) @staticmethod def load(filename:str) -> \"DictDatabase\": \"\"\"Load db from a file Args: filename (str): Name of the file to load Returns: DictDatabase: the db \"\"\" with open(filename, \"rb\") as f: db = pickle.load(f) return db def dumps(self) -> bytes: \"\"\"Generate pickle byte stream Returns: _type_: _description_ \"\"\" return pickle.dumps(self) @staticmethod def loads(binary_data: bytes) -> \"DictDatabase\": \"\"\"Load a binary string representing a database Initially only unpickles the data Args: binary_data (str): String of data to unpickle Returns: Model object \"\"\" return pickle.loads(binary_data) dumps() Generate pickle byte stream Returns: Name Type Description _type_ bytes description Source code in simstring\\database\\dict.py def dumps(self) -> bytes: \"\"\"Generate pickle byte stream Returns: _type_: _description_ \"\"\" return pickle.dumps(self) load(filename) staticmethod Load db from a file Parameters: Name Type Description Default filename str Name of the file to load required Returns: Name Type Description DictDatabase DictDatabase the db Source code in simstring\\database\\dict.py @staticmethod def load(filename:str) -> \"DictDatabase\": \"\"\"Load db from a file Args: filename (str): Name of the file to load Returns: DictDatabase: the db \"\"\" with open(filename, \"rb\") as f: db = pickle.load(f) return db loads(binary_data) staticmethod Load a binary string representing a database Initially only unpickles the data Parameters: Name Type Description Default binary_data str String of data to unpickle required Returns: Type Description DictDatabase Model object Source code in simstring\\database\\dict.py @staticmethod def loads(binary_data: bytes) -> \"DictDatabase\": \"\"\"Load a binary string representing a database Initially only unpickles the data Args: binary_data (str): String of data to unpickle Returns: Model object \"\"\" return pickle.loads(binary_data) save(filename) Save the database to a file as defined by filename. Parameters: Name Type Description Default filename str Filename to save the db at. Should include file extention. required Returns: Type Description None Source code in simstring\\database\\dict.py def save(self, filename:str): \"\"\"Save the database to a file as defined by filename. Args: filename: Filename to save the db at. Should include file extention. Returns: None \"\"\" with open(filename, \"wb\") as f: pickle.dump(self, f) PyMongo based database Bases: BaseDatabase Source code in simstring\\database\\mongo.py class MongoDatabase(BaseDatabase): def __init__(self, feature_extractor, host=(os.environ[\"MONGO_HOST\"] if \"MONGO_HOST\" in os.environ else 'localhost'), port=27017, database='simstring'): self.feature_extractor = feature_extractor client = MongoClient(host, port) db = client[database] self.collection = db.strings self.ensure_index() def add(self, string): features = self.feature_extractor.features(string) self.collection.insert_one({\"string\": string, \"features\": features, \"size\": len(features)}) def all(self): return list(map(lambda x: x['string'], self.all_documents())) def all_documents(self): return list(self.collection.find()) def ensure_index(self): self.collection.create_index('size') self.collection.create_index('features') def lookup_strings_by_feature_set_size_and_feature(self, size, feature): documents = list(self.collection.find({\"size\": size, \"features\": feature})) return set(list(map(lambda x: x['string'], documents))) def reset_collection(self): self.collection.remove() self.ensure_index()","title":"Database"},{"location":"database/#database","text":"","title":"Database"},{"location":"database/#dict-based-database","text":"Bases: BaseDatabase Source code in simstring\\database\\dict.py class DictDatabase(BaseDatabase): def __init__(self, feature_extractor): self.feature_extractor = feature_extractor self.strings: List[str] = [] self.feature_set_size_to_string_map: Dict[int, Set[str]] = defaultdict(set) # 3.10 and up only self.feature_set_size_and_feature_to_string_map: dict = defaultdict(defaultdict_set) def add(self, string: str): features = self.feature_extractor.features(string) size = len(features) self.strings.append(string) self.feature_set_size_to_string_map[size].add(string) for feature in features: self.feature_set_size_and_feature_to_string_map[size][feature].add(string) def all(self) -> List[str]: return self.strings def lookup_strings_by_feature_set_size_and_feature(self, size: int, feature: str) -> Set[str]: return self.feature_set_size_and_feature_to_string_map[size][feature] def min_feature_size(self) -> int: return min(self.feature_set_size_to_string_map.keys()) def max_feature_size(self) -> int: return max(self.feature_set_size_to_string_map.keys()) # def __getstate__(self): # \"\"\"To pickle the object\"\"\" # return self.__dict__ # def __setstate__(self, d): # \"\"\"To unpickle the object\"\"\" # self.__dict__ = d def save(self, filename:str): \"\"\"Save the database to a file as defined by filename. Args: filename: Filename to save the db at. Should include file extention. Returns: None \"\"\" with open(filename, \"wb\") as f: pickle.dump(self, f) @staticmethod def load(filename:str) -> \"DictDatabase\": \"\"\"Load db from a file Args: filename (str): Name of the file to load Returns: DictDatabase: the db \"\"\" with open(filename, \"rb\") as f: db = pickle.load(f) return db def dumps(self) -> bytes: \"\"\"Generate pickle byte stream Returns: _type_: _description_ \"\"\" return pickle.dumps(self) @staticmethod def loads(binary_data: bytes) -> \"DictDatabase\": \"\"\"Load a binary string representing a database Initially only unpickles the data Args: binary_data (str): String of data to unpickle Returns: Model object \"\"\" return pickle.loads(binary_data)","title":"Dict based database"},{"location":"database/#simstring.database.dict.DictDatabase.dumps","text":"Generate pickle byte stream Returns: Name Type Description _type_ bytes description Source code in simstring\\database\\dict.py def dumps(self) -> bytes: \"\"\"Generate pickle byte stream Returns: _type_: _description_ \"\"\" return pickle.dumps(self)","title":"dumps()"},{"location":"database/#simstring.database.dict.DictDatabase.load","text":"Load db from a file Parameters: Name Type Description Default filename str Name of the file to load required Returns: Name Type Description DictDatabase DictDatabase the db Source code in simstring\\database\\dict.py @staticmethod def load(filename:str) -> \"DictDatabase\": \"\"\"Load db from a file Args: filename (str): Name of the file to load Returns: DictDatabase: the db \"\"\" with open(filename, \"rb\") as f: db = pickle.load(f) return db","title":"load()"},{"location":"database/#simstring.database.dict.DictDatabase.loads","text":"Load a binary string representing a database Initially only unpickles the data Parameters: Name Type Description Default binary_data str String of data to unpickle required Returns: Type Description DictDatabase Model object Source code in simstring\\database\\dict.py @staticmethod def loads(binary_data: bytes) -> \"DictDatabase\": \"\"\"Load a binary string representing a database Initially only unpickles the data Args: binary_data (str): String of data to unpickle Returns: Model object \"\"\" return pickle.loads(binary_data)","title":"loads()"},{"location":"database/#simstring.database.dict.DictDatabase.save","text":"Save the database to a file as defined by filename. Parameters: Name Type Description Default filename str Filename to save the db at. Should include file extention. required Returns: Type Description None Source code in simstring\\database\\dict.py def save(self, filename:str): \"\"\"Save the database to a file as defined by filename. Args: filename: Filename to save the db at. Should include file extention. Returns: None \"\"\" with open(filename, \"wb\") as f: pickle.dump(self, f)","title":"save()"},{"location":"database/#pymongo-based-database","text":"Bases: BaseDatabase Source code in simstring\\database\\mongo.py class MongoDatabase(BaseDatabase): def __init__(self, feature_extractor, host=(os.environ[\"MONGO_HOST\"] if \"MONGO_HOST\" in os.environ else 'localhost'), port=27017, database='simstring'): self.feature_extractor = feature_extractor client = MongoClient(host, port) db = client[database] self.collection = db.strings self.ensure_index() def add(self, string): features = self.feature_extractor.features(string) self.collection.insert_one({\"string\": string, \"features\": features, \"size\": len(features)}) def all(self): return list(map(lambda x: x['string'], self.all_documents())) def all_documents(self): return list(self.collection.find()) def ensure_index(self): self.collection.create_index('size') self.collection.create_index('features') def lookup_strings_by_feature_set_size_and_feature(self, size, feature): documents = list(self.collection.find({\"size\": size, \"features\": feature})) return set(list(map(lambda x: x['string'], documents))) def reset_collection(self): self.collection.remove() self.ensure_index()","title":"PyMongo based database"},{"location":"features/","text":"Feature extractors Bases: BaseFeatureExtractor Source code in simstring\\feature_extractor\\character_ngram.py class CharacterNgramFeatureExtractor(BaseFeatureExtractor): def __init__(self, n:int=2): self.n = n def features(self, string: str) -> List[str]: list_of_ngrams = self._each_cons('$' * (self.n - 1) + string + '$' * (self.n - 1), self.n) return self.uniquify_list(list_of_ngrams) Bases: BaseFeatureExtractor Source code in simstring\\feature_extractor\\word_ngram.py class WordNgramFeatureExtractor(BaseFeatureExtractor): def __init__(self, n=2, splitter=\" \"): self.n = n self.splitter = splitter def features(self, text: str) -> List[str]: # Split text by white space. # If you want to extract words from text in more complicated way or using your favorite library like NLTK, please implement in your own. words = text.split(self.splitter) return self._words_ngram(words, self.n, SENTINAL_CHAR) Bases: BaseFeatureExtractor Source code in simstring\\feature_extractor\\mecab_ngram.py class MecabNgramFeatureExtractor(BaseFeatureExtractor): def __init__(self, n=2, user_dic_path='', sys_dic_path=''): self.n = n self.mecab = MecabTokenizer(user_dic_path, sys_dic_path) def features(self, text: str) -> List[str]: words = [x.surface() for x in self.mecab.tokenize(text)] return self._words_ngram(words, self.n, SENTINAL_CHAR)","title":"Features"},{"location":"features/#feature-extractors","text":"Bases: BaseFeatureExtractor Source code in simstring\\feature_extractor\\character_ngram.py class CharacterNgramFeatureExtractor(BaseFeatureExtractor): def __init__(self, n:int=2): self.n = n def features(self, string: str) -> List[str]: list_of_ngrams = self._each_cons('$' * (self.n - 1) + string + '$' * (self.n - 1), self.n) return self.uniquify_list(list_of_ngrams) Bases: BaseFeatureExtractor Source code in simstring\\feature_extractor\\word_ngram.py class WordNgramFeatureExtractor(BaseFeatureExtractor): def __init__(self, n=2, splitter=\" \"): self.n = n self.splitter = splitter def features(self, text: str) -> List[str]: # Split text by white space. # If you want to extract words from text in more complicated way or using your favorite library like NLTK, please implement in your own. words = text.split(self.splitter) return self._words_ngram(words, self.n, SENTINAL_CHAR) Bases: BaseFeatureExtractor Source code in simstring\\feature_extractor\\mecab_ngram.py class MecabNgramFeatureExtractor(BaseFeatureExtractor): def __init__(self, n=2, user_dic_path='', sys_dic_path=''): self.n = n self.mecab = MecabTokenizer(user_dic_path, sys_dic_path) def features(self, text: str) -> List[str]: words = [x.surface() for x in self.mecab.tokenize(text)] return self._words_ngram(words, self.n, SENTINAL_CHAR)","title":"Feature extractors"},{"location":"measure/","text":"Measure The measure defines the formula by which the distance between strings is measured. Use as: from simstring.measure import CosineMeasure, JaccardMeasure, OverlapMeasure, DiceMeasure But be carefull, they are not identical to the normal definitions of these measures. Cosine Measure is different to scipy.spatial.distance.cosine as it works on strings and not vectors. Jaccard distance does not discard duplicates in its sets, unlike in the normally used definition. This means that 'fooo' is seen as more different from 'fo' than 'foo', which is a more useful way of lookng at the string difference, but is not the usual definition of the distance as implimanted by scipy.spatial.distance.jaccard or wikipedia or any public calculator . Cosine Measure Bases: BaseMeasure Source code in simstring\\measure\\cosine.py class CosineMeasure(BaseMeasure): def min_feature_size(self, query_size:int, alpha:float) -> int: return int(math.ceil(alpha * alpha * query_size)) def max_feature_size(self, query_size:int, alpha:float) -> int: return int(math.floor(query_size / (alpha * alpha))) def minimum_common_feature_count(self, query_size: int, y_size:int , alpha: float) -> int: return int(math.ceil(alpha * math.sqrt(query_size * y_size))) def similarity(self, X: Iterable[str], Y: Iterable[str]) -> float: return len(set(X) & set(Y)) / math.sqrt(len(set(X)) * len(set(Y))) Jaccard Measure Bases: BaseMeasure Source code in simstring\\measure\\jaccard.py class JaccardMeasure(BaseMeasure): def min_feature_size(self, query_size:int, alpha:float) -> int: return int(math.ceil(alpha * query_size)) def max_feature_size(self, query_size:int, alpha:float) -> int: return int(math.floor(query_size / alpha)) def minimum_common_feature_count(self, query_size: int, y_size:int , alpha: float) -> int: return int(math.ceil(alpha * (query_size + y_size) * 1.0 / (1 + alpha))) def similarity(self, X: Iterable[str], Y: Iterable[str]) -> float: return len(set(X) & set(Y)) * 1.0 / len(set(X) | set(Y)) OverlapMeasures Bases: BaseMeasure Source code in simstring\\measure\\overlap.py class OverlapMeasure(BaseMeasure): def __init__(self, db=None, maxsize: int=100) -> None: super().__init__() if db: self.maxsize = db.max_feature_size() else: self.maxsize = maxsize def min_feature_size(self, query_size, alpha) -> int: # return 1 # Not sure the below isn't sufficient return math.floor(query_size*alpha) or 1 def max_feature_size(self, query_size, alpha) -> int: return self.maxsize def minimum_common_feature_count(self, query_size: int, y_size: int, alpha: float) -> int: return int(math.ceil(alpha * min(query_size, y_size))) def similarity(self, X: Iterable[str], Y: Iterable[str]) -> int: return min(len(set(X)), len(set(Y))) Bases: BaseMeasure Source code in simstring\\measure\\overlap.py class LeftOverlapMeasure(BaseMeasure): def __init__(self, db=None, maxsize: int=100) -> None: super().__init__() if db: self.maxsize = db.max_feature_size() else: self.maxsize = maxsize def min_feature_size(self, query_size, alpha) -> int: return math.floor(query_size*alpha) or 1 def max_feature_size(self, query_size, alpha) -> int: return self.maxsize def minimum_common_feature_count(self, query_size: int, y_size: int, alpha: float) -> int: return math.floor(query_size*alpha) or 1 def similarity(self, X: Iterable[str], Y: Iterable[str]) -> float: return 1- len(set(X) - set(Y) )/len(set(X)) DiceMeasure Bases: BaseMeasure Source code in simstring\\measure\\dice.py class DiceMeasure(BaseMeasure): def min_feature_size(self, query_size:int, alpha:float) -> int: return int(math.ceil(alpha * 1.0 / (2 - alpha) * query_size)) def max_feature_size(self, query_size:int, alpha:float) -> int: return int(math.floor((2 - alpha) * query_size * 1.0 / alpha)) def minimum_common_feature_count(self, query_size: int, y_size:int , alpha: float) -> int: return int(math.ceil(0.5 * alpha * query_size * y_size)) def similarity(self, X: Iterable[str], Y: Iterable[str]) -> float: return len(set(X) & set(Y)) * 2.0 / (len(set(X)) + len(set(Y)))","title":"Measure"},{"location":"measure/#measure","text":"The measure defines the formula by which the distance between strings is measured. Use as: from simstring.measure import CosineMeasure, JaccardMeasure, OverlapMeasure, DiceMeasure But be carefull, they are not identical to the normal definitions of these measures. Cosine Measure is different to scipy.spatial.distance.cosine as it works on strings and not vectors. Jaccard distance does not discard duplicates in its sets, unlike in the normally used definition. This means that 'fooo' is seen as more different from 'fo' than 'foo', which is a more useful way of lookng at the string difference, but is not the usual definition of the distance as implimanted by scipy.spatial.distance.jaccard or wikipedia or any public calculator .","title":"Measure"},{"location":"measure/#cosine-measure","text":"Bases: BaseMeasure Source code in simstring\\measure\\cosine.py class CosineMeasure(BaseMeasure): def min_feature_size(self, query_size:int, alpha:float) -> int: return int(math.ceil(alpha * alpha * query_size)) def max_feature_size(self, query_size:int, alpha:float) -> int: return int(math.floor(query_size / (alpha * alpha))) def minimum_common_feature_count(self, query_size: int, y_size:int , alpha: float) -> int: return int(math.ceil(alpha * math.sqrt(query_size * y_size))) def similarity(self, X: Iterable[str], Y: Iterable[str]) -> float: return len(set(X) & set(Y)) / math.sqrt(len(set(X)) * len(set(Y)))","title":"Cosine Measure"},{"location":"measure/#jaccard-measure","text":"Bases: BaseMeasure Source code in simstring\\measure\\jaccard.py class JaccardMeasure(BaseMeasure): def min_feature_size(self, query_size:int, alpha:float) -> int: return int(math.ceil(alpha * query_size)) def max_feature_size(self, query_size:int, alpha:float) -> int: return int(math.floor(query_size / alpha)) def minimum_common_feature_count(self, query_size: int, y_size:int , alpha: float) -> int: return int(math.ceil(alpha * (query_size + y_size) * 1.0 / (1 + alpha))) def similarity(self, X: Iterable[str], Y: Iterable[str]) -> float: return len(set(X) & set(Y)) * 1.0 / len(set(X) | set(Y))","title":"Jaccard Measure"},{"location":"measure/#overlapmeasures","text":"Bases: BaseMeasure Source code in simstring\\measure\\overlap.py class OverlapMeasure(BaseMeasure): def __init__(self, db=None, maxsize: int=100) -> None: super().__init__() if db: self.maxsize = db.max_feature_size() else: self.maxsize = maxsize def min_feature_size(self, query_size, alpha) -> int: # return 1 # Not sure the below isn't sufficient return math.floor(query_size*alpha) or 1 def max_feature_size(self, query_size, alpha) -> int: return self.maxsize def minimum_common_feature_count(self, query_size: int, y_size: int, alpha: float) -> int: return int(math.ceil(alpha * min(query_size, y_size))) def similarity(self, X: Iterable[str], Y: Iterable[str]) -> int: return min(len(set(X)), len(set(Y))) Bases: BaseMeasure Source code in simstring\\measure\\overlap.py class LeftOverlapMeasure(BaseMeasure): def __init__(self, db=None, maxsize: int=100) -> None: super().__init__() if db: self.maxsize = db.max_feature_size() else: self.maxsize = maxsize def min_feature_size(self, query_size, alpha) -> int: return math.floor(query_size*alpha) or 1 def max_feature_size(self, query_size, alpha) -> int: return self.maxsize def minimum_common_feature_count(self, query_size: int, y_size: int, alpha: float) -> int: return math.floor(query_size*alpha) or 1 def similarity(self, X: Iterable[str], Y: Iterable[str]) -> float: return 1- len(set(X) - set(Y) )/len(set(X))","title":"OverlapMeasures"},{"location":"measure/#dicemeasure","text":"Bases: BaseMeasure Source code in simstring\\measure\\dice.py class DiceMeasure(BaseMeasure): def min_feature_size(self, query_size:int, alpha:float) -> int: return int(math.ceil(alpha * 1.0 / (2 - alpha) * query_size)) def max_feature_size(self, query_size:int, alpha:float) -> int: return int(math.floor((2 - alpha) * query_size * 1.0 / alpha)) def minimum_common_feature_count(self, query_size: int, y_size:int , alpha: float) -> int: return int(math.ceil(0.5 * alpha * query_size * y_size)) def similarity(self, X: Iterable[str], Y: Iterable[str]) -> float: return len(set(X) & set(Y)) * 2.0 / (len(set(X)) + len(set(Y)))","title":"DiceMeasure"},{"location":"searcher/","text":"Searcher Source code in simstring\\searcher.py class Searcher: def __init__(self, db, measure) -> None: \"\"\"Searcher class This is the main way of interacting with the simsting search. Args: db (database): A database, can be a dict or mongo one as defined by the `database` modeule measure (measure): The similarity measure as defined by `measure` \"\"\" self.db = db self.measure = measure self.feature_extractor = db.feature_extractor self.lookup_strings_result: dict = defaultdict(dict) def search(self, query_string: str, alpha: float) -> List[str]: features = self.feature_extractor.features(query_string) lf = len(features) min_feature_size = self.measure.min_feature_size(lf, alpha) max_feature_size = self.measure.max_feature_size(lf, alpha) results = [] for candidate_feature_size in range(min_feature_size, max_feature_size + 1): tau = self.__min_overlap(lf, candidate_feature_size, alpha) results.extend(self.__overlap_join(features, tau, candidate_feature_size)) return results def ranked_search(self, query_string: str, alpha: float) -> List[Tuple[float, str]]: results = self.search(query_string, alpha) features = self.feature_extractor.features(query_string) results_with_score = list( map( lambda x: [ self.measure.similarity( features, self.feature_extractor.features(x) ), x, ], results, ) ) # Why change the signature? is this used in ASAP? # return { # name: score # for score, name in sorted(results_with_score, key=lambda x: (-x[0], x[1])) # } return [(score, name) for score, name in sorted(results_with_score, key=lambda x: (-x[0], x[1])) ] def __min_overlap( self, query_size: int, candidate_feature_size: int, alpha: float ) -> int: return self.measure.minimum_common_feature_count( query_size, candidate_feature_size, alpha ) def __overlap_join(self, features, tau, candidate_feature_size: int) -> List[str]: query_feature_size = len(features) features_mapped_to_lookup_strings_sets = { x: self.__lookup_strings_by_feature_set_size_and_feature( candidate_feature_size, x ) for x in features } features.sort(key=lambda x: len(features_mapped_to_lookup_strings_sets[x])) #candidate_string_to_matched_count : Dict[str,int] = defaultdict(int) # Only in 3.10 and later candidate_string_to_matched_count : Dict = defaultdict(int) results = [] for feature in features[0 : query_feature_size - tau + 1]: for s in features_mapped_to_lookup_strings_sets[feature]: candidate_string_to_matched_count[s] += 1 # The next loop does not run for tau = 1, hence candidates are never checked, while all satisfies the criteria if tau == 1: results = list(candidate_string_to_matched_count.keys()) for ( candidate, candidate_match_count, ) in candidate_string_to_matched_count.items(): for i in range(query_feature_size - tau + 1, query_feature_size): feature = features[i] if candidate in features_mapped_to_lookup_strings_sets[feature]: candidate_match_count += 1 if candidate_match_count >= tau: results.append(candidate) break remaining_feature_count = query_feature_size - i - 1 if candidate_match_count + remaining_feature_count < tau: break return results def __lookup_strings_by_feature_set_size_and_feature(self, feature_size: int, feature: str): if feature not in self.lookup_strings_result[feature_size]: self.lookup_strings_result[feature_size][ feature ] = self.db.lookup_strings_by_feature_set_size_and_feature( feature_size, feature ) return self.lookup_strings_result[feature_size][feature] __init__(db, measure) Searcher class This is the main way of interacting with the simsting search. Parameters: Name Type Description Default db database A database, can be a dict or mongo one as defined by the database modeule required measure measure The similarity measure as defined by measure required Source code in simstring\\searcher.py def __init__(self, db, measure) -> None: \"\"\"Searcher class This is the main way of interacting with the simsting search. Args: db (database): A database, can be a dict or mongo one as defined by the `database` modeule measure (measure): The similarity measure as defined by `measure` \"\"\" self.db = db self.measure = measure self.feature_extractor = db.feature_extractor self.lookup_strings_result: dict = defaultdict(dict)","title":"Searcher"},{"location":"searcher/#searcher","text":"Source code in simstring\\searcher.py class Searcher: def __init__(self, db, measure) -> None: \"\"\"Searcher class This is the main way of interacting with the simsting search. Args: db (database): A database, can be a dict or mongo one as defined by the `database` modeule measure (measure): The similarity measure as defined by `measure` \"\"\" self.db = db self.measure = measure self.feature_extractor = db.feature_extractor self.lookup_strings_result: dict = defaultdict(dict) def search(self, query_string: str, alpha: float) -> List[str]: features = self.feature_extractor.features(query_string) lf = len(features) min_feature_size = self.measure.min_feature_size(lf, alpha) max_feature_size = self.measure.max_feature_size(lf, alpha) results = [] for candidate_feature_size in range(min_feature_size, max_feature_size + 1): tau = self.__min_overlap(lf, candidate_feature_size, alpha) results.extend(self.__overlap_join(features, tau, candidate_feature_size)) return results def ranked_search(self, query_string: str, alpha: float) -> List[Tuple[float, str]]: results = self.search(query_string, alpha) features = self.feature_extractor.features(query_string) results_with_score = list( map( lambda x: [ self.measure.similarity( features, self.feature_extractor.features(x) ), x, ], results, ) ) # Why change the signature? is this used in ASAP? # return { # name: score # for score, name in sorted(results_with_score, key=lambda x: (-x[0], x[1])) # } return [(score, name) for score, name in sorted(results_with_score, key=lambda x: (-x[0], x[1])) ] def __min_overlap( self, query_size: int, candidate_feature_size: int, alpha: float ) -> int: return self.measure.minimum_common_feature_count( query_size, candidate_feature_size, alpha ) def __overlap_join(self, features, tau, candidate_feature_size: int) -> List[str]: query_feature_size = len(features) features_mapped_to_lookup_strings_sets = { x: self.__lookup_strings_by_feature_set_size_and_feature( candidate_feature_size, x ) for x in features } features.sort(key=lambda x: len(features_mapped_to_lookup_strings_sets[x])) #candidate_string_to_matched_count : Dict[str,int] = defaultdict(int) # Only in 3.10 and later candidate_string_to_matched_count : Dict = defaultdict(int) results = [] for feature in features[0 : query_feature_size - tau + 1]: for s in features_mapped_to_lookup_strings_sets[feature]: candidate_string_to_matched_count[s] += 1 # The next loop does not run for tau = 1, hence candidates are never checked, while all satisfies the criteria if tau == 1: results = list(candidate_string_to_matched_count.keys()) for ( candidate, candidate_match_count, ) in candidate_string_to_matched_count.items(): for i in range(query_feature_size - tau + 1, query_feature_size): feature = features[i] if candidate in features_mapped_to_lookup_strings_sets[feature]: candidate_match_count += 1 if candidate_match_count >= tau: results.append(candidate) break remaining_feature_count = query_feature_size - i - 1 if candidate_match_count + remaining_feature_count < tau: break return results def __lookup_strings_by_feature_set_size_and_feature(self, feature_size: int, feature: str): if feature not in self.lookup_strings_result[feature_size]: self.lookup_strings_result[feature_size][ feature ] = self.db.lookup_strings_by_feature_set_size_and_feature( feature_size, feature ) return self.lookup_strings_result[feature_size][feature]","title":"Searcher"},{"location":"searcher/#simstring.searcher.Searcher.__init__","text":"Searcher class This is the main way of interacting with the simsting search. Parameters: Name Type Description Default db database A database, can be a dict or mongo one as defined by the database modeule required measure measure The similarity measure as defined by measure required Source code in simstring\\searcher.py def __init__(self, db, measure) -> None: \"\"\"Searcher class This is the main way of interacting with the simsting search. Args: db (database): A database, can be a dict or mongo one as defined by the `database` modeule measure (measure): The similarity measure as defined by `measure` \"\"\" self.db = db self.measure = measure self.feature_extractor = db.feature_extractor self.lookup_strings_result: dict = defaultdict(dict)","title":"__init__()"}]}