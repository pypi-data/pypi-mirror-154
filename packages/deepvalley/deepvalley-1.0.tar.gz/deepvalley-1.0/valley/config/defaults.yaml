LAUNCH:
  DEBUG: False
  HOST_NUM: 1
  GPU_PER_HOST: 1
  HOST_RANK: 0
  DIST_URL: localhost
  LOCAL_RANK: 0
  GLOBAL_RANK: 0
  WORLD_SIZE: 0

EXPERIMENT:
  ROOT_DIR:
    
  CONFIG_DIR: config
  CHECKPOINT_DIR: checkpoint
  LOG_DIR: log
  TENSORBOARD_DIR: tensorboard

  CFG_FILE: config.yaml

  LOG_FILE: .
  LOG_PERIOD: 100 # in iteration

  EVAL_PERIOD: 1  # in epoch
  EVAL_START: 0   # in epoch

  EVAL_ONLY: False
  RESUME: False
  MAX_EPOCH: 40
  SEED: 1

ENGINE:
  NAME: DefaultTrainer
  
DATASET:
  TRAIN: 
    NAME: dataset_train
    DATA_ROOT: ./data/
    FORMAT_TYPE: 

  TEST: 
    NAME: dataset_valid
    DATA_ROOT: ./data/
    FORMAT_TYPE: 

DATALOADER:
  TRAIN: DefaultDataLoader
  TEST: DefaultDataLoader
  BATCH_SIZE: 128
  WORKER_NUM: 0

EVALUATOR:
  NAME: MNISTEvaluator

MODEL:
  DEVICE: "cuda"
  META_ARCHITECTURE: "LeNet_CLS"
  WEIGHTS: ""

  # Values to be used for image normalization (BGR order, since INPUT.FORMAT defaults to BGR).
  # To train on images of different number of channels, just set different mean & std.
  # Default values are the mean pixel value from ImageNet: [103.53, 116.28, 123.675]
  PIXEL_MEAN: [103.530, 116.280, 123.675]

  # When using pre-trained models in Detectron1 or any MSRA models,
  # std has been absorbed into its conv1 weights, so the std needs to be set 1.
  # Otherwise, you can use [57.375, 57.120, 58.395] (ImageNet std)
  PIXEL_STD: [1.0, 1.0, 1.0]

# ---------------------------------------------------------------------------- #
# Backbone options
# ---------------------------------------------------------------------------- #
  BACKBONE:
    NAME: "build_lenet_backbone"


SOLVER:
  MAX_ITER: 100000
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  BASE_LR: 0.1
  MOMENTUM: 0.9
  NESTEROV: False
  WEIGHT_DECAY: 0.0001

  # The weight decay that's applied to parameters of normalization layers
  # (typically the affine transformation)
  WEIGHT_DECAY_NORM: 0.0
  GAMMA: 0.1
  
  # The iteration number to decrease learning rate by GAMMA.
  STEPS: [30000, 50000]
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 1000
  WARMUP_METHOD: "linear"

  # Save a checkpoint after every this number of iterations
  CHECKPOINT_PERIOD: 1


  # The reference number of workers (GPUs) this config is meant to train with.
  # It takes no effect when set to 0.
  # With a non-zero value, it will be used by DefaultTrainer to compute a desired
  # per-worker batch size, and then scale the other related configs (total batch size,
  # learning rate, etc) to match the per-worker batch size.
  # See documentation of `DefaultTrainer.auto_scale_workers` for details:
  REFERENCE_WORLD_SIZE: 0

  # Gradient clipping
  CLIP_GRADIENTS:
    ENABLED: False

    # Type of gradient clipping, currently 2 values are supported:
    # - "value": the absolute values of elements of each gradients are clipped
    # - "norm": the norm of the gradient for each parameter is clipped thus
    #   affecting all elements in the parameter
    CLIP_TYPE: "value"

    # Maximum absolute value used for clipping gradients
    CLIP_VALUE: 1.0

    # Floating point number p for L-p norm to be used with the "norm"
    # gradient clipping type; for L-inf, please specify .inf
    NORM_TYPE: 2.0

  # Enable automatic mixed precision for training
  # Note that this does not change model's inference behavior.
  # To use AMP in inference, run inference under autocast()
  AMP:
    ENABLED: True
