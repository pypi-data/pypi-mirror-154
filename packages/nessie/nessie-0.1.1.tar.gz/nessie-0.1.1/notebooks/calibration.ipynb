{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Calibration\n",
    "\n",
    "Several model-based AED methods, for instance *Classification Uncertainty*, directly leverage probability estimates provided by the backing model.\n",
    "Therefore, it is of interest whether models output class probability distributions which are accurate.\n",
    "For instance, if a model predicts 100 instances and states for all 80% confidence, then the accuracy should be around 0.8.\n",
    "If this is the case for a model, then it is called **calibrated**.\n",
    "There are several approaches for post-hoc calibration that can be applied after a model for AED has been trained.\n",
    "In `nessie`, we use the [netcal](https://fabiankueppers.github.io/calibration-framework/build/html/index.html) library for calibration.\n",
    "Their documentation also provides a list of calibration methods to try.\n",
    "From our evaluation, *Platt Scaling* (also called *Logistic Calibration*) worked well.\n",
    "In our experiments, calibration can statistically improve AED performance and more often than not has no large negative side effects. \n",
    "Please refer to our paper for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Usage\n",
    "\n",
    "Calibration is normally trained on a holdout set.\n",
    "As we already perform cross-validation, we use the holdout set both for training the calibration and for predicting annotation errors.\n",
    "While this would not be optimal if we are interested in generalizing calibrated probabilities to unseen data, we are more interested in downstream task performance.\n",
    "Using an additional fold per round would be theoretically more sound.\n",
    "But our preliminary experiments show that it has the issue of reducing the available training data and thereby hurts the error detection performance more than the calibration helps.\n",
    "Using the same fold for both calibration and applying \\ac{aed}, however, improves overall task performance which is what matters in our special task setting.\n",
    "We do not leak the values for the downstream tasks (whether an instance is labeled wrong or not) but only the labels for the primary task.\n",
    "`nessie` provides helper methods for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import ireval\n",
    "\n",
    "from netcal.scaling import LogisticCalibration\n",
    "\n",
    "from nessie.calibration import CalibrationCallback, CalibratorWrapper\n",
    "from nessie.detectors import ClassificationUncertainty\n",
    "from nessie.dataloader import load_example_text_classification_data\n",
    "from nessie.helper import CrossValidationHelper\n",
    "from nessie.models.text import DummyTextClassifier\n",
    "\n",
    "ds = load_example_text_classification_data()\n",
    "\n",
    "model = DummyTextClassifier()\n",
    "detector = ClassificationUncertainty()\n",
    "\n",
    "# Without calibration\n",
    "\n",
    "calibrator = CalibratorWrapper(LogisticCalibration())\n",
    "calibration_callback = CalibrationCallback(calibrator)\n",
    "\n",
    "cv = CrossValidationHelper()\n",
    "\n",
    "result_uncalibrated = cv.run(ds.texts, ds.noisy_labels, model)\n",
    "scores_uncalibrated = detector.score(ds.noisy_labels, result_uncalibrated.probabilities, result_uncalibrated.le)\n",
    "\n",
    "# With calibration\n",
    "\n",
    "calibrator = CalibratorWrapper(LogisticCalibration())\n",
    "calibration_callback = CalibrationCallback(calibrator)\n",
    "\n",
    "cv = CrossValidationHelper()\n",
    "cv.add_callback(calibration_callback)\n",
    "\n",
    "result_calibrated = cv.run(ds.texts, ds.noisy_labels, model)\n",
    "scores_calibrated = detector.score(ds.noisy_labels, result_calibrated.probabilities, result_calibrated.le)\n",
    "\n",
    "# Evaluation\n",
    "flags = ds.flags\n",
    "ap_uncalibrated = ireval.average_precision(flags, scores_uncalibrated)\n",
    "ap_calibrated = ireval.average_precision(flags, scores_calibrated)\n",
    "\n",
    "print(f\"AP uncalibrated: {ap_uncalibrated}\")\n",
    "print(f\"AP calibrated: {ap_calibrated}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}