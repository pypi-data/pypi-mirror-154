from typing import Optional
from sys import stdout

from DeepPhysX.Core.Pipelines.BasePipeline import BasePipeline
from DeepPhysX.Core.Manager.Manager import Manager
from DeepPhysX.Core.Network.BaseNetworkConfig import BaseNetworkConfig
from DeepPhysX.Core.Dataset.BaseDatasetConfig import BaseDatasetConfig
from DeepPhysX.Core.Environment.BaseEnvironmentConfig import BaseEnvironmentConfig
from DeepPhysX.Core.Utils.progressbar import Progressbar


class BaseTrainer(BasePipeline):
    """
    | BaseTrainer is a pipeline defining the training process of an artificial neural network.
    | It provides a highly tunable learning process that can be used with any machine learning library.

    :param BaseNetworkConfig network_config: Specialisation containing the parameters of the network manager
    :param BaseDatasetConfig dataset_config: Specialisation containing the parameters of the dataset manager
    :param Optional[BaseEnvironmentConfig] environment_config: Specialisation containing the parameters of the
                                                               environment manager
    :param str session_name: Name of the newly created directory if session_dir is not defined
    :param Optional[str] session_dir: Name of the directory in which to write all the necessary data
    :param bool new_session: Define the creation of new directories to store data
    :param int nb_epochs: Number of epochs
    :param int nb_batches: Number of batches
    :param int batch_size: Size of a batch
    :param bool debug: If True, main training features will not be launched
    """

    def __init__(self,
                 network_config: BaseNetworkConfig,
                 dataset_config: BaseDatasetConfig,
                 environment_config: Optional[BaseEnvironmentConfig] = None,
                 session_name: str = 'default',
                 session_dir: Optional[str] = None,
                 new_session: bool = True,
                 nb_epochs: int = 0,
                 nb_batches: int = 0,
                 batch_size: int = 0,
                 debug: bool = False):

        if environment_config is None and dataset_config.dataset_dir is None:
            print("BaseTrainer: You have to give me a dataset source (existing dataset directory or simulation to "
                  "create data on the fly")
            quit(0)

        BasePipeline.__init__(self,
                              network_config=network_config,
                              dataset_config=dataset_config,
                              environment_config=environment_config,
                              session_name=session_name,
                              session_dir=session_dir,
                              pipeline='training')

        # Training variables
        self.nb_epochs = nb_epochs
        self.id_epoch = 0
        self.nb_batches = nb_batches
        self.batch_size = batch_size
        self.id_batch = 0
        self.nb_samples = nb_batches * batch_size * nb_epochs
        self.loss_dict = None

        # Tell if data is recording while predicting (output is recorded only if input too)
        self.record_data = {'input': True, 'output': True}

        self.debug = debug
        if not self.debug:
            self.progress_counter = 0
            self.digits = ['{' + f':0{len(str(self.nb_epochs))}d' + '}',
                           '{' + f':0{len(str(self.nb_batches))}d' + '}']
            id_epoch, nb_epoch = self.digits[0].format(0), self.digits[0].format(self.nb_epochs)
            id_batch, nb_batch = self.digits[1].format(0), self.digits[1].format(self.nb_batches)
            self.progress_bar = Progressbar(start=0, stop=self.nb_batches * self.nb_epochs, c='orange',
                                            title=f'Epoch n째{id_epoch}/{nb_epoch} - Batch n째{id_batch}/{nb_batch} ')

        self.manager = Manager(pipeline=self,
                               network_config=self.network_config,
                               dataset_config=dataset_config,
                               environment_config=self.environment_config,
                               session_name=session_name,
                               session_dir=session_dir,
                               new_session=new_session,
                               batch_size=batch_size)

        self.manager.save_info_file()

    def execute(self) -> None:
        """
        | Main function of the training process \"execute\" call the functions associated with the learning process.
        | Each of the called functions are already implemented so one can start a basic training.
        | Each of the called function can also be rewritten via inheritance to provide more specific / complex training
          process.
        """

        self.train_begin()
        while self.epoch_condition():
            self.epoch_begin()
            while self.batch_condition():
                self.batch_begin()
                self.optimize()
                self.batch_count()
                self.batch_end()
            self.epoch_count()
            self.epoch_end()
            self.save_network()
        self.train_end()

    def optimize(self) -> None:
        """
        | Pulls data from the manager and run a prediction and optimizer step.
        """

        self.manager.get_data(self.id_epoch, self.batch_size)
        _, self.loss_dict = self.manager.optimize_network()

    def save_network(self) -> None:
        """
        | Registers the network weights and biases in the corresponding directory (session_name/network or
          session_dir/network)
        """

        self.manager.save_network()

    def train_begin(self) -> None:
        """
        | Called once at the very beginning of the training process.
        | Allows the user to run some pre-computations.
        """

        pass

    def train_end(self) -> None:
        """
        | Called once at the very end of the training process.
        | Allows the user to run some post-computations.
        """

        self.manager.close()

    def epoch_begin(self) -> None:
        """
        | Called one at the start of each epoch.
        | Allows the user to run some pre-epoch computations.
        """

        self.id_batch = 0

    def epoch_end(self) -> None:
        """
        | Called one at the end of each epoch.
        | Allows the user to run some post-epoch computations.
        """

        self.manager.stats_manager.add_train_epoch_loss(self.loss_dict['loss'], self.id_epoch)

    def epoch_condition(self) -> bool:
        """
        | Condition that characterize the end of the training process.
        
        :return: False if the training needs to stop.
        """

        return self.id_epoch < self.nb_epochs

    def epoch_count(self) -> None:
        """
        | Allows user for custom update of epochs count.
        """

        self.id_epoch += 1

    def batch_begin(self) -> None:
        """
        | Called one at the start of each batch.
        | Allows the user to run some pre-batch computations.
        """

        if not self.debug:
            stdout.write("\033[K")
            self.progress_counter += 1
            id_epoch, nb_epoch = self.digits[0].format(self.id_epoch + 1), self.digits[0].format(self.nb_epochs)
            id_batch, nb_batch = self.digits[1].format(self.id_batch + 1), self.digits[1].format(self.nb_batches)
            self.progress_bar.title = f'Epoch n째{id_epoch}/{nb_epoch} - Batch n째{id_batch}/{nb_batch} '
            self.progress_bar.print(counts=self.progress_counter)

    def batch_end(self) -> None:
        """
        | Called one at the start of each batch.
        | Allows the user to run some post-batch computations.
        """

        self.manager.stats_manager.add_train_batch_loss(self.loss_dict['loss'],
                                                        self.id_epoch * self.nb_batches + self.id_batch)
        for key in self.loss_dict.keys():
            if key != 'loss':
                self.manager.stats_manager.add_custom_scalar(tag=key,
                                                             value=self.loss_dict[key],
                                                             count=self.id_epoch * self.nb_batches + self.id_batch)

    def batch_condition(self) -> bool:
        """
        | Condition that characterize the end of the epoch.
        
        :return: False if the epoch needs to stop.
        """

        return self.id_batch < self.nb_batches

    def batch_count(self):
        """
        | Allows user for custom update of batches count.
        
        :return:
        """

        self.id_batch += 1

    def __str__(self) -> str:
        """
        :return: str Contains training information about the training process
        """

        description = "\n"
        description += f"# {self.__class__.__name__}\n"
        description += f"    Session directory: {self.manager.session_dir}\n"
        description += f"    Number of epochs: {self.nb_epochs}\n"
        description += f"    Number of batches per epoch: {self.nb_batches}\n"
        description += f"    Number of samples per batch: {self.batch_size}\n"
        description += f"    Number of samples per epoch: {self.nb_batches * self.batch_size}\n"
        description += f"    Total: Number of batches : {self.nb_batches * self.nb_epochs}\n"
        description += f"           Number of samples : {self.nb_samples}\n"
        return description
